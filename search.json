[
  {
    "objectID": "prep-for-class-09.html",
    "href": "prep-for-class-09.html",
    "title": "Preparation for Class 09",
    "section": "",
    "text": "This online resource by Jennifer Bryan, Jim Hester, Shannon Pileggi, and E. David Aja emphasizes some basic, yet valuable tips and tricks for R users, such as:\nCheck for Updates: Keep R and RStudio updated to benefit from the latest features and fixes.\nUse Projects: Start with an R project for organized file management and simplified paths.\nManage Packages: Regularly check and update packages, keeping only those necessary.\nOrganize Paths and Files: Set structured directories and use relative paths for consistency.\nPractice Debugging: Cultivate debugging habits for efficient troubleshooting.\nStart with Blank Slate and Restart Often: Regularly restarting prevents lingering data or settings from causing errors, promoting reproducible, isolated environments.\nFor more details, see the guide."
  },
  {
    "objectID": "prep-for-class-09.html#what-they-forgot-to-teach-you-about-r",
    "href": "prep-for-class-09.html#what-they-forgot-to-teach-you-about-r",
    "title": "Preparation for Class 09",
    "section": "",
    "text": "This online resource by Jennifer Bryan, Jim Hester, Shannon Pileggi, and E. David Aja emphasizes some basic, yet valuable tips and tricks for R users, such as:\nCheck for Updates: Keep R and RStudio updated to benefit from the latest features and fixes.\nUse Projects: Start with an R project for organized file management and simplified paths.\nManage Packages: Regularly check and update packages, keeping only those necessary.\nOrganize Paths and Files: Set structured directories and use relative paths for consistency.\nPractice Debugging: Cultivate debugging habits for efficient troubleshooting.\nStart with Blank Slate and Restart Often: Regularly restarting prevents lingering data or settings from causing errors, promoting reproducible, isolated environments.\nFor more details, see the guide."
  },
  {
    "objectID": "prep-for-class-05.html",
    "href": "prep-for-class-05.html",
    "title": "Preparation for Class 05",
    "section": "",
    "text": "Big data offers immense potential for insights and innovation, but it also introduces significant risks. One of the key pitfalls is Big Data Hubris—the assumption that large datasets can replace traditional data collection and analysis methods. The case of Google Flu Trends (GFT) discussed in Lazer et al.’s article in Science (2014) exemplifies this.\nGFT used Google search data to predict flu outbreaks but ultimately failed due to overconfidence in the data’s predictive capabilities. Despite having millions of search queries, the model missed critical trends, such as the 2009 H1N1 pandemic, and consistently overestimated flu prevalence compared to CDC data. This failure illustrates that data volume alone does not guarantee accuracy; attention must be paid to the quality, validity, and reliability of data sources.\nAnother key issue is algorithm dynamics that may lead to endogeneity. Changes in the underlying algorithms and user behaviors, such as modifications in Google’s search engine, can alter the data-generating process, leading to inaccurate predictions. In the GFT case, these changes contributed to persistent errors, further complicating the model’s ability to predict flu trends accurately. As a result, big data analytics can be vulnerable when it relies too heavily on ever-evolving, non-static data sources."
  },
  {
    "objectID": "prep-for-class-05.html#big-data-analytics-pitfalls",
    "href": "prep-for-class-05.html#big-data-analytics-pitfalls",
    "title": "Preparation for Class 05",
    "section": "",
    "text": "Big data offers immense potential for insights and innovation, but it also introduces significant risks. One of the key pitfalls is Big Data Hubris—the assumption that large datasets can replace traditional data collection and analysis methods. The case of Google Flu Trends (GFT) discussed in Lazer et al.’s article in Science (2014) exemplifies this.\nGFT used Google search data to predict flu outbreaks but ultimately failed due to overconfidence in the data’s predictive capabilities. Despite having millions of search queries, the model missed critical trends, such as the 2009 H1N1 pandemic, and consistently overestimated flu prevalence compared to CDC data. This failure illustrates that data volume alone does not guarantee accuracy; attention must be paid to the quality, validity, and reliability of data sources.\nAnother key issue is algorithm dynamics that may lead to endogeneity. Changes in the underlying algorithms and user behaviors, such as modifications in Google’s search engine, can alter the data-generating process, leading to inaccurate predictions. In the GFT case, these changes contributed to persistent errors, further complicating the model’s ability to predict flu trends accurately. As a result, big data analytics can be vulnerable when it relies too heavily on ever-evolving, non-static data sources."
  },
  {
    "objectID": "prep-for-class-05.html#overfitting-and-overparameterization",
    "href": "prep-for-class-05.html#overfitting-and-overparameterization",
    "title": "Preparation for Class 05",
    "section": "Overfitting and Overparameterization",
    "text": "Overfitting and Overparameterization\nOverfitting occurs when a model is tailored too closely to the specific data it was trained on, capturing noise rather than meaningful patterns (signal). GFT demonstrated overfitting by correlating flu-related search terms with a small set of historical flu cases. This resulted in the inclusion of spurious variables, such as high school basketball searches, which were unrelated to flu outbreaks. Although these variables fit past data well, they failed to predict future flu activity, highlighting the dangers of overfitting in big data models.\nSimilarly, overparameterization—using too many variables in a model—exacerbates overfitting by making the model overly sensitive to small variations in the training data. GFT’s reliance on millions of search terms without sufficient filtering or theory-driven reasoning caused the model to generalize poorly. This led to overestimations of flu prevalence for over two years.\nTo mitigate these issues, it is essential to combine big data methods with traditional approaches. For example, integrating GFT data with CDC reports and recalibrating the model based on real-world inputs improved predictive accuracy. This approach shows that while big data can provide valuable insights, it must be supplemented with well-established analytical methods to avoid the pitfalls of overfitting, overparameterization, and dynamic data sources.\n\nReferences\nLazer, D., Kennedy, R., King, G., & Vespignani, A. (2014). The Parable of Google Flu: Traps in Big Data Analysis. Science (American Association for the Advancement of Science), 343(6176), 1203–1205. https://doi.org/10.1126/science.1248506\nOverfitting. (n.d.). In Wikipedia. Retrieved September 25, 2024, from https://en.wikipedia.org/wiki/Overfitting"
  },
  {
    "objectID": "prep-for-class-05.html#note-on-hadley-wickhams-keynote-lecture-at-embl",
    "href": "prep-for-class-05.html#note-on-hadley-wickhams-keynote-lecture-at-embl",
    "title": "Preparation for Class 05",
    "section": "Note on Hadley Wickham’s Keynote Lecture at EMBL",
    "text": "Note on Hadley Wickham’s Keynote Lecture at EMBL\nIn his keynote lecture at EMBL, Hadley Wickham highlights several key technologies and techniques essential for data visualization and data science through the lens of the R programming environment.\n\n\n\nEvolution of three threads of R packages\n\n\nSource: Hadley Wickham, Keynote Lecture at EMLB\n\nTidyverse and Data Science Workflow\nWickham introduces the concept of the tidyverse, a collection of R packages, such as ggplot2, dplyr, tidyr, and purrr, that simplify data manipulation, transformation, and visualization. His focus is on how these tools make data analysis more efficient by promoting consistency in data formatting and handling.\n\n\n\nTidyverse\n\n\nSource: Hadley Wickham, Keynote Lecture at EMLB\n“Tidy” data, a central principle in his work, ensures that each variable is stored in its own column, and each observation gets its own row. This format makes data easier to analyze and visualize.\n\n\n\nR Packages for Data Visualizaton, Reshaping, and Manipulation\n\n\nSource: Hadley Wickham, Keynote Lecture at EMLB\n\n\nOrthogonal Components\nWickham emphasizes the importance of breaking down data into orthogonal components, meaning that different variables or features of the data should be analyzed separately to gain clear insights. This is akin to separating the dimensions of a dataset to better visualize and understand its structure. He demonstrates this concept with visualizations, such as using GDP per capita versus life expectancy in the well-known Gapminder World dataset and its visualization by Hans Rosling.\n\n\nImportance of Code\nA significant theme in Wickham’s presentation is the power of code. He stresses that code is not only for computation but also serves as text, which makes it readable and reproducible. This focus on the reproducibility of data analysis is vital for transparency and verification in research, allowing others to follow and replicate analyses."
  },
  {
    "objectID": "hackathon.html",
    "href": "hackathon.html",
    "title": "Hackathon",
    "section": "",
    "text": "#&gt; Load the necessary libraries and data set  \nlibrary(ggplot2)  \nlibrary(scales)  \nlibrary(RColorBrewer)  \ndata(diamonds)  \n\n\n\n\n#&gt; Define a sequential palette from RColorBrewer package  \nblues_palette &lt;- brewer.pal(9, \"Blues\")  \ncustom_palette &lt;- blues_palette[5:9] # select particular colors  \n\n\n#&gt; Create the horizontal bar chart with facets  \nggplot(data = diamonds, aes(x = clarity, fill = cut)) +  \n  geom_bar(color = \"black\") +  \n  facet_wrap(~ cut, nrow = 1) +  \n  scale_y_continuous(labels = label_number(scale = 1/1000, suffix = \"K\")) +  \n  coord_flip() +  \n  scale_fill_manual(values = custom_palette) +  \n  theme_minimal(base_family = \"sans\") +  \n  labs(  \n    title = \"Number of Diamonds by Clarity Across Cut Levels\",  \n    x = \"Clarity\",  \n    y = \"Number of Diamonds\",  \n    fill = \"Cut\"  \n  ) +  \n  theme(  \n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),  \n    axis.text.y = element_text(size = 8),  \n    axis.text.x = element_text(size = 8),  \n    strip.text = element_text(size = 11),  \n    legend.position = \"none\",  \n    text = element_text(family = \"sans\")  \n  )  \n\n\n\n\n\n\n\n\n\n\n\n\n#&gt; Filter the dataset for the required cuts  \nfiltered_diamonds &lt;- subset(diamonds, cut %in% c(\"Very Good\", \"Ideal\"))  \n\n\n#&gt; Create the plot  \n#&gt; The key issue was to make the left bars overlap over/on top of the rigt bars!  \nggplot(filtered_diamonds, aes(x = color)) +  \n  geom_bar(data = subset(filtered_diamonds, cut == \"Ideal\"),  \n           aes(y = after_stat(count), fill = cut),  \n           position = position_nudge(x = 0.15),  \n           width = 0.5,  \n           alpha = 0.7) +  #&gt; Right bars (Ideal)  \n  geom_bar(data = subset(filtered_diamonds, cut == \"Very Good\"),  \n           aes(y = after_stat(count), fill = cut),  \n           width = 0.5) +  #&gt; Left bars (Very Good) on top  \n  scale_y_continuous(labels = scales::unit_format(unit = \"k\", scale = 1e-3)) +  #&gt; Format y-axis labels  \n  labs(  \n    title = \"Number of Diamonds by Color across Selected Cut Quality Levels\",  \n    x = \"Color\",  \n    y = \"Number of Diamonds\",  \n    fill = \"Selected Cut Quality Levels\"  \n  ) +  \n  scale_fill_manual(values = c(\"Very Good\" = \"#66c2a5\", \"Ideal\" = \"#fc8d62\")) +  #&gt; Custom colors  \n  theme_minimal(base_family = \"sans\") +  \n  theme(  \n    plot.title = element_text(face = \"bold\", size = 14, hjust = 0.5),  #&gt; Centered, bold, and larger title  \n    axis.text.y = element_text(size = 10),  #&gt; Adjust y-axis text size  \n    legend.position = \"top\"  #&gt; Adjust legend position  \n  )"
  },
  {
    "objectID": "hackathon.html#creating-the-horizontal-bar-chart-3",
    "href": "hackathon.html#creating-the-horizontal-bar-chart-3",
    "title": "Hackathon",
    "section": "",
    "text": "#&gt; Define a sequential palette from RColorBrewer package  \nblues_palette &lt;- brewer.pal(9, \"Blues\")  \ncustom_palette &lt;- blues_palette[5:9] # select particular colors  \n\n\n#&gt; Create the horizontal bar chart with facets  \nggplot(data = diamonds, aes(x = clarity, fill = cut)) +  \n  geom_bar(color = \"black\") +  \n  facet_wrap(~ cut, nrow = 1) +  \n  scale_y_continuous(labels = label_number(scale = 1/1000, suffix = \"K\")) +  \n  coord_flip() +  \n  scale_fill_manual(values = custom_palette) +  \n  theme_minimal(base_family = \"sans\") +  \n  labs(  \n    title = \"Number of Diamonds by Clarity Across Cut Levels\",  \n    x = \"Clarity\",  \n    y = \"Number of Diamonds\",  \n    fill = \"Cut\"  \n  ) +  \n  theme(  \n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),  \n    axis.text.y = element_text(size = 8),  \n    axis.text.x = element_text(size = 8),  \n    strip.text = element_text(size = 11),  \n    legend.position = \"none\",  \n    text = element_text(family = \"sans\")  \n  )"
  },
  {
    "objectID": "hackathon.html#creating-column-chart-4",
    "href": "hackathon.html#creating-column-chart-4",
    "title": "Hackathon",
    "section": "",
    "text": "#&gt; Filter the dataset for the required cuts  \nfiltered_diamonds &lt;- subset(diamonds, cut %in% c(\"Very Good\", \"Ideal\"))  \n\n\n#&gt; Create the plot  \n#&gt; The key issue was to make the left bars overlap over/on top of the rigt bars!  \nggplot(filtered_diamonds, aes(x = color)) +  \n  geom_bar(data = subset(filtered_diamonds, cut == \"Ideal\"),  \n           aes(y = after_stat(count), fill = cut),  \n           position = position_nudge(x = 0.15),  \n           width = 0.5,  \n           alpha = 0.7) +  #&gt; Right bars (Ideal)  \n  geom_bar(data = subset(filtered_diamonds, cut == \"Very Good\"),  \n           aes(y = after_stat(count), fill = cut),  \n           width = 0.5) +  #&gt; Left bars (Very Good) on top  \n  scale_y_continuous(labels = scales::unit_format(unit = \"k\", scale = 1e-3)) +  #&gt; Format y-axis labels  \n  labs(  \n    title = \"Number of Diamonds by Color across Selected Cut Quality Levels\",  \n    x = \"Color\",  \n    y = \"Number of Diamonds\",  \n    fill = \"Selected Cut Quality Levels\"  \n  ) +  \n  scale_fill_manual(values = c(\"Very Good\" = \"#66c2a5\", \"Ideal\" = \"#fc8d62\")) +  #&gt; Custom colors  \n  theme_minimal(base_family = \"sans\") +  \n  theme(  \n    plot.title = element_text(face = \"bold\", size = 14, hjust = 0.5),  #&gt; Centered, bold, and larger title  \n    axis.text.y = element_text(size = 10),  #&gt; Adjust y-axis text size  \n    legend.position = \"top\"  #&gt; Adjust legend position  \n  )"
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Generative Art is a process of algorithmically generating new ideas, forms, shapes, colors or patterns. First, you create rules that provide boundaries for the creation process. Then a computer follows those rules to produce new works on your behalf.\nSource: https://aiartists.org/generative-art-design\n\n\n\n\n\nExample 1\n\n\nGenerative Art by Manolo Gamboa Naon, an Argentinian artist who uses algorithmic tools including Processing to create art.\nSource: https://aiartists.org/generative-art-design\n\n\n\nExample 2\n\n\nGenerative art by Anders Hoff. This is part of his project “Inconvergent”, which explores the complex behavior that emerges from systems with simple rules.\nSource:https://aiartists.org/generative-art-design\n\n\n\nExample 3\n\n\nHarold Cohen. Untitled Computer Drawing (1982). Tate © Harold Cohen\nSource: https://www.tate.org.uk/art/art-terms/g/generative-art"
  },
  {
    "objectID": "assignment1.html#generative-art",
    "href": "assignment1.html#generative-art",
    "title": "Assignment 1",
    "section": "",
    "text": "Generative Art is a process of algorithmically generating new ideas, forms, shapes, colors or patterns. First, you create rules that provide boundaries for the creation process. Then a computer follows those rules to produce new works on your behalf.\nSource: https://aiartists.org/generative-art-design\n\n\n\n\n\nExample 1\n\n\nGenerative Art by Manolo Gamboa Naon, an Argentinian artist who uses algorithmic tools including Processing to create art.\nSource: https://aiartists.org/generative-art-design\n\n\n\nExample 2\n\n\nGenerative art by Anders Hoff. This is part of his project “Inconvergent”, which explores the complex behavior that emerges from systems with simple rules.\nSource:https://aiartists.org/generative-art-design\n\n\n\nExample 3\n\n\nHarold Cohen. Untitled Computer Drawing (1982). Tate © Harold Cohen\nSource: https://www.tate.org.uk/art/art-terms/g/generative-art"
  },
  {
    "objectID": "assignment1.html#fall-became-summer",
    "href": "assignment1.html#fall-became-summer",
    "title": "Assignment 1",
    "section": "Fall Became Summer",
    "text": "Fall Became Summer\n\n\n\nFall R-Plot with a Different Color"
  },
  {
    "objectID": "assignment1.html#example-of-chart",
    "href": "assignment1.html#example-of-chart",
    "title": "Assignment 1",
    "section": "Example of Chart",
    "text": "Example of Chart\n\n\n\nChart in Public Administration\n\n\nSource: Jensen, U. T., Salomonsen, H. H., & Moynihan, D. P. (2018). Communicating the Vision: How Face-to-Face Dialogue Facilitates Transformational Leadership. Public Administration Review, 78(3), 350–361. https://doi.org/10.1111/puar.12922\nPapers in public administration, my major, typically include tables and black-and-white charts, such as the one from an article I recently read for another class, which is shown above.\nThe chart clearly shows the marginal effect of transformational leadership on mission valence for different levels of face-to-face dialogue. This makes it easy to understand how the three variables interact.\nThe axes are well-labeled, making it clear what is being measured and how to interpret the data. The notes additionally help to read the chart, especially in case of anything is not fully clear from the chart per se.\nIncluding 95% confidence intervals helps to convey the uncertainty around the estimates, which is crucial for making informed decisions in public administration.\nHowever, it is not fully clear why different values of face-to-face dialogue seem to be grouped or unequally distributed along the x-axis. Extra labels between the “rare” and “extensive” values might help clarify this.\nAdditionally, the following could be recommended for other contexts beyond Academia or when publishing in another journal with more reader friendly standards. Using different colors or line styles could make the chart more visually appealing and easier to distinguish between different elements.\nMoreover, if this chart is being used in a digital format, adding interactive elements (like tooltips or clickable points) could enhance user engagement and understanding."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This webpage was created for posting assignments for my Data Visualization class."
  },
  {
    "objectID": "prep-for-class-04.html",
    "href": "prep-for-class-04.html",
    "title": "Preparation for Class 04",
    "section": "",
    "text": "Notes on Edward Tufte’s Presentation “The Future of Data Analysis”\n\nA purpose of a data display is to assist reasoning about its content.\nShowing and thinking become as one.\nEdward Tufte\n\nIn his 2016 presentation, Edward Tufte explores the evolving landscape of data visualization and its impact on understanding complex information. He emphasizes the importance of clarity, precision, and efficiency in data presentation. He argues that the future of data analysis hinges on the effective communication of complex data through visual means, highlighting the need for simplicity without sacrificing depth.\nTufte underscores the necessity of integrating statistical thinking with visual design. He advocates for visualizations that reveal data accurately and meaningfully, facilitating deeper insights.\n\n\n\nVisualization of Measles Cases in the U.S. before and after Measles Vaccination\n\n\nSource: Edward Tufte, “The Future of Data Analysis”\n\n\n\nMap of Swiss Alps\n\n\nSource: Edward Tufte, “The Future of Data Analysis”\nHe also promotes the idea of maximizing data density in visualizations, allowing viewers to glean more information with less cognitive effort. Tufte sees successful examples of this approach in Goggle maps providing rich information to billions of people across the globe who seem not to find it overwhelming at all.\n\n\n\nMapping of a 2.5’’ Video of a Soccer Match Visualizing the Goal\n\n\nSource: Edward Tufte, “The Future of Data Analysis”\nThus, Tufte’s presentation calls for a future in data analysis that prioritizes the seamless integration of design and analytical rigor, ensuring that visualizations not only inform but also inspire deeper understanding.\nAt the same time, he addresses the issue of data and data analysis quality, discussing the crisis in data analysis. Tufte cites various studies that explore the problematic status of many research conclusions and issues with the replication of published research results (e.g., Ioannidis, J. P. A. Why Most Published Research Findings Are False; Lazer et al. The Parable of Goggle Flu: Traps in Big Data Analysis). The speaker cites John W. Tukey: “The data may not contain the answer. And, if you torture the data long enough, it will tell you anything.”\n\n\n\nPlot of Unemployment vs. Inflation Rates in the U.S. as an Example of Selective Data Analysis\n\n\nSource: Edward Tufte, “The Future of Data Analysis”\n\n\n\nDiscrepancies between Google Flu Forecast and CDC Actual Flu Cases Data\n\n\nSource: Edward Tufte, “The Future of Data Analysis”\nTufte calls for a deeper look at data that we analyze and its connection to the real world. Not uncommonly, the way data is produced can be compromised, with “tilts” (subtle biases or directional shifts that analysts, intentionally or unintentionally, may introduce when interpreting data) aimed at generating publishable findings.\nHe emphasizes honesty in research design, analysis, and representation of research findings, and envisions a greater role of data forensics in the future to ensure validity of research on human behavior.\nTufte sees the future of confirmatory data analysis in pre-specified methods to prevent data search in order to generate “findings” (according to practices of CDC). He also urges to “go out in the field and observe how data is made” and “look at the real world, not just its representations.”"
  },
  {
    "objectID": "prep-for-class-07.html",
    "href": "prep-for-class-07.html",
    "title": "Preparation for Class 07",
    "section": "",
    "text": "Literate programming is a programming paradigm introduced by Donald Knuth (1984) that combines code and documentation in a single, coherent document. It focuses on writing code in a way that is understandable to humans, emphasizing that code should not only instruct the computer but also communicate clearly with other people, including the programmer’s future self.\nAs Knuth noted, “Let us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do (Knuth, 1984, In Bowers & Voors, 2016, p. 834).”\nThe main task is to write explanations, using natural language, that describe the logic and structure of the program, while embedding the code itself within those explanations. This approach makes the program easier to read and understand, both for others who might need to review or modify the code later or oneself.\nFor example, a typical literate programming document will include:\n\nExplanatory Text: Sections of prose that describe what the code is intended to do, the purpose of different functions, and the logic behind decisions.\nCode Chunks: Sections of actual code that implement the logic described in the text. These chunks can be extracted and compiled separately if needed.\n\nTools like R Markdown or Jupyter Notebooks facilitate literate programming by allowing users to blend documentation with code execution directly in a structured way.\nLiterate programming is particularly useful in research and scientific contexts where reproducibility is critical, as it allows a single document to serve as both a research paper and an executable program.\nMy only concern regarding literate programming is that it may require a bit more time to systematically employ this approach. As a result, there is always a temptation to diverge from it and not to write detailed comments when being overwhelmed."
  },
  {
    "objectID": "prep-for-class-07.html#literate-programming",
    "href": "prep-for-class-07.html#literate-programming",
    "title": "Preparation for Class 07",
    "section": "",
    "text": "Literate programming is a programming paradigm introduced by Donald Knuth (1984) that combines code and documentation in a single, coherent document. It focuses on writing code in a way that is understandable to humans, emphasizing that code should not only instruct the computer but also communicate clearly with other people, including the programmer’s future self.\nAs Knuth noted, “Let us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do (Knuth, 1984, In Bowers & Voors, 2016, p. 834).”\nThe main task is to write explanations, using natural language, that describe the logic and structure of the program, while embedding the code itself within those explanations. This approach makes the program easier to read and understand, both for others who might need to review or modify the code later or oneself.\nFor example, a typical literate programming document will include:\n\nExplanatory Text: Sections of prose that describe what the code is intended to do, the purpose of different functions, and the logic behind decisions.\nCode Chunks: Sections of actual code that implement the logic described in the text. These chunks can be extracted and compiled separately if needed.\n\nTools like R Markdown or Jupyter Notebooks facilitate literate programming by allowing users to blend documentation with code execution directly in a structured way.\nLiterate programming is particularly useful in research and scientific contexts where reproducibility is critical, as it allows a single document to serve as both a research paper and an executable program.\nMy only concern regarding literate programming is that it may require a bit more time to systematically employ this approach. As a result, there is always a temptation to diverge from it and not to write detailed comments when being overwhelmed."
  },
  {
    "objectID": "prep-for-class-07.html#four-pillars-of-data-visualization",
    "href": "prep-for-class-07.html#four-pillars-of-data-visualization",
    "title": "Preparation for Class 07",
    "section": "Four Pillars of Data Visualization",
    "text": "Four Pillars of Data Visualization\nThe four pillars of data visualization—distribution, relationship, composition, and comparison—are fundamental principles that guide effective visual communication of data:\n\nDistribution: Focuses on how data is spread across a range. It highlights the frequency and patterns in the data, such as the spread of values or the concentration of certain results. Common charts for visualizing distribution include histograms, box plots, and density plots.\nRelationship: This pillar emphasizes the connections or correlations between two or more variables. It is useful for identifying patterns, trends, or interactions among variables. Scatter plots and bubble charts are commonly used to visualize relationships.\nComposition: Refers to how different parts make up a whole. It’s ideal for showing proportions and how various elements contribute to a total. Pie charts, stacked bar charts, and treemaps are often used for visualizing composition.\nComparison: This pillar is about comparing values across categories, time periods, or groups to identify differences and similarities. Bar charts, line charts, and area charts are typically used to make comparisons clear and easy to understand.\n\nEach pillar helps to effectively convey different aspects of data, guiding the selection of visualization types to best represent the insights within the data."
  },
  {
    "objectID": "prep-for-class-07.html#music-visualization",
    "href": "prep-for-class-07.html#music-visualization",
    "title": "Preparation for Class 07",
    "section": "Music Visualization",
    "text": "Music Visualization\nIn the suggested video, Stephen Malinowski visualized the fugue from J. S. Bach’s Fantasia and Fugue in A minor, BWV 904. Malinowski arranged the music for two instruments (brass and woodwind) using music production software and then created a video.\nHe used the following elements in his visualization:\n\nThe vertical position of geometrical forms is associated with the height of sounds (pitch).\nDifferent shapes are associated with the instruments: circles refer to brass, horizontal bars – to woodwind).\nSize of circles and length of bars mean the duration of sounds: the bigger the circles and the longer the bars – the longer the sounds.\nFinally, color refers to different voices in this polyphonic musical piece, such as soprano, alto, tenor, and bass.\n\nMusic visualization has quite a long history tracing back to the 19th century. For example, Alexander Scriabin, a composer from Russia, included a part for a “light keyboard,” or luce, in his symphonic piece Prometheus: The Poem of Fire (1910). Scriabin had a condition believed to be synesthesia, which led him to associate specific musical keys with particular colors.\nThus, the light keyboard was intended to project colored lights corresponding to the notes being played. Each pitch would have a corresponding color, creating a visual display during the performance. However, technology at the time could not fully realize his vision, so many early performances omitted this element."
  }
]